{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jVKXrMJ8ODv"
      },
      "id": "5jVKXrMJ8ODv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZYe9-7sg8ThF",
        "outputId": "46dc56b4-7372-4731-a9a2-8c8a9348bbf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZYe9-7sg8ThF",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1fb1c10e",
      "metadata": {
        "id": "1fb1c10e",
        "outputId": "9894dfb2-79f3-43f3-e5d1-c3e50dd59c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 0 Jan  3 16:35 '/content/drive/MyDrive/Colab Notebooks/helper_functions.py'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "\n",
        "\n",
        "!ls -l '/content/drive/MyDrive/Colab Notebooks/helper_functions.py'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import set_seeds"
      ],
      "metadata": {
        "id": "YpqufY_K95v7",
        "outputId": "34847af9-5776-4e74-8a4d-348081d72b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "id": "YpqufY_K95v7",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'set_seeds' from 'helper_functions' (/content/drive/MyDrive/Colab Notebooks/helper_functions.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-073e6351f33c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_seeds' from 'helper_functions' (/content/drive/MyDrive/Colab Notebooks/helper_functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e97c71",
      "metadata": {
        "id": "12e97c71"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de25b1a",
      "metadata": {
        "id": "0de25b1a"
      },
      "outputs": [],
      "source": [
        "# 1. Get pretrained weights for ViT-Base\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "\n",
        "# 2. Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
        "\n",
        "# 3. Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# 4. Change the classifier head\n",
        "class_names = ['daisy','dandelion']\n",
        "\n",
        "set_seeds()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
        "# pretrained_vit # uncomment for model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3feaa42",
      "metadata": {
        "id": "e3feaa42"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=pretrained_vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c73ec300",
      "metadata": {
        "id": "c73ec300"
      },
      "source": [
        "#### Notice how only the output layer is trainable, where as, all of the rest of the layers are untrainable (frozen)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8cc699",
      "metadata": {
        "id": "ac8cc699"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/train'\n",
        "test_dir = 'D:/tranformer_env/L-3_image_classification_using_ViT/custom_dataset/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91175306",
      "metadata": {
        "id": "91175306"
      },
      "source": [
        "Remember, if you're going to use a pretrained model, it's generally important to ensure your own custom data is transformed/formatted in the same way the data the original model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aa777b",
      "metadata": {
        "id": "05aa777b"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088971e6",
      "metadata": {
        "id": "088971e6"
      },
      "source": [
        "## And now we've got transforms ready, we can turn our images into DataLoaders using the create_dataloaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d49225b",
      "metadata": {
        "id": "5d49225b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS\n",
        "):\n",
        "\n",
        "  # Use ImageFolder to create dataset(s)\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9037c8a5",
      "metadata": {
        "id": "9037c8a5"
      },
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir=test_dir,\n",
        "                                                                                                     transform=pretrained_vit_transforms,\n",
        "                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c5ba74",
      "metadata": {
        "id": "10c5ba74"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of the pretrained ViT feature extractor model\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit,\n",
        "                                      train_dataloader=train_dataloader_pretrained,\n",
        "                                      test_dataloader=test_dataloader_pretrained,\n",
        "                                      optimizer=optimizer,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      epochs=10,\n",
        "                                      device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945149b4",
      "metadata": {
        "id": "945149b4"
      },
      "source": [
        "pretrained ViT performed far better than our custom ViT model trained from scratch (in the same amount of time).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aae16a8",
      "metadata": {
        "id": "2aae16a8"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79922fd1",
      "metadata": {
        "id": "79922fd1"
      },
      "source": [
        "## That's the power of transfer learning!\n",
        "\n",
        "We managed to get outstanding results with the same model architecture, except our custom implementation was trained from scratch (worse performance) and this feature extractor model has the power of pretrained weights from ImageNet behind it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce4427b",
      "metadata": {
        "id": "6ce4427b"
      },
      "source": [
        "# Let's make Prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe89b4e9",
      "metadata": {
        "id": "fe89b4e9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = \"test_img.jpg\"\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e7d64d0",
      "metadata": {
        "id": "5e7d64d0"
      },
      "outputs": [],
      "source": [
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = \"test_1.jpg\"\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7d0f9e",
      "metadata": {
        "id": "eb7d0f9e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}